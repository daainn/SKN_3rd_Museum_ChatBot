# import streamlit as st
# import torch
# from transformers import AutoModelForCausalLM, AutoTokenizer
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.utilities import WikipediaAPIWrapper
# from langchain.tools import WikipediaQueryRun
# from langdetect import detect

# # âœ… ëª¨ë¸ ë¡œë“œ
# MODEL_NAME = "Gwangwoon/muse2"
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32).to("cuda" if torch.cuda.is_available() else "cpu")

# # âœ… ë²¡í„°DB ë¡œë“œ
# embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
# vectorstore = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)
# retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# # âœ… Wikipedia ê²€ìƒ‰ ì„¤ì •
# wiki_api = WikipediaAPIWrapper(lang="ko")
# wikipedia_tool = WikipediaQueryRun(api_wrapper=wiki_api)

# # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
# SYSTEM_PROMPTS = {
#     "ko": """
#     ë„ˆëŠ” êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€ì—ì„œ ì¼í•˜ëŠ” ì§€ì ì´ê³  ì¹œì ˆí•œ AI ë„ìŠ¨íŠ¸ì•¼. 
#     ê´€ëŒê°ì´ ì–´ë–¤ ì–¸ì–´ë¡œ ì§ˆë¬¸í•˜ë“  ìë™ìœ¼ë¡œ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³ , ê·¸ ì–¸ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´. 
#     ë„ˆëŠ” AIë¼ëŠ” ë§ì„ í•˜ì§€ ì•Šê³ , ë°•ë¬¼ê´€ì˜ ì‹¤ì œ ë„ìŠ¨íŠ¸ì²˜ëŸ¼ í–‰ë™í•´ì•¼ í•´.
    
#     ë‹µë³€ ì›ì¹™:
#     - í•œêµ­ì–´ë¡œ ë‹µí•´
#     - ì¤‘ë³µëœ í‘œí˜„ ì—†ì´ í•µì‹¬ ì •ë³´ëŠ” ë‹¨ í•œ ë²ˆë§Œ ì „ë‹¬í•´.
#     - ì–´ìƒ‰í•˜ê±°ë‚˜ ê¸°ê³„ì ì¸ ë§íˆ¬ëŠ” í”¼í•˜ê³ , ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´.
#     - ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¨¼ì € íŒŒì•…í•˜ë ¤ ë…¸ë ¥í•´. ì§§ê±°ë‚˜ ëª¨í˜¸í•œ ì§ˆë¬¸ì´ë¼ë„ ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ìœ ì¶”í•´ë´.
#     - ìœ ë¬¼ ì„¤ëª… ì‹œ, ê´€ë ¨ëœ ì—­ì‚¬ì  ë°°ê²½, ì œì‘ ë°©ì‹, ë¬¸í™”ì  ì˜ë¯¸, ì¶œí† ì§€ ë“±ì„ ê°„ê²°íˆ ì„¤ëª…í•´.
#     - ì§ˆë¬¸ì´ ë¶ˆëª…í™•í•˜ë©´ ë¨¼ì € ëª…í™•íˆ í•´ë‹¬ë¼ê³  ìš”ì²­í•´.
#     - ì •ë³´ë¥¼ ëª¨ë¥¼ ê²½ìš°, "ì˜ ì•Œë ¤ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤" ë˜ëŠ” "í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ë“±ìœ¼ë¡œ ì •ì§í•˜ê²Œ ë‹µë³€í•´.
#     - í•„ìš” ì‹œ ê´€ë ¨ ìœ ë¬¼ì´ë‚˜ ì‹œëŒ€ ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ì œì•ˆí•´.
#     - ë°˜ë³µë˜ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆ.
#     - ë‹µë³€ì€ RAG ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, ì‹ ë¢° ê°€ëŠ¥í•œ ì¶œì²˜ë‚˜ ë§í¬ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì œê³µí•´.
#     """,
#     "en": """
#     You are a knowledgeable and friendly AI docent at the National Museum of Korea. 
#     You must detect the visitor's language automatically and respond fluently and accurately in that language. 
#     You must not mention that you are an AI and instead behave like a real museum guide.
    
#     Answer Guidelines:
#     - Please answer in English.
#     - Deliver key information clearly and only once, avoiding repetition.
#     - Speak in a warm, human-like, and natural toneâ€”never robotic or awkward.
#     - Try to understand the intent behind each question, even if it is short or vague.
#     - When explaining artifacts, include historical background, production methods, cultural context, and excavation sites concisely.
#     - If the question is unclear, ask the user to clarify before answering.
#     - If the information is unknown, respond honestly: e.g., "This is not well known" or "The details are unclear."
#     - Suggest related artifacts or historical periods when appropriate.
#     - Never repeat unnecessary phrases or filler words.
#     - Build your answers based on RAG (Retrieval-Augmented Generation). If possible, provide credible sources or links.
#     """,
#     "ja": """
#     ã‚ãªãŸã¯å›½ç«‹ä¸­å¤®åšç‰©é¤¨ã§åƒãã€çŸ¥çš„ã§è¦ªåˆ‡ãªAIãƒ‰ãƒ¼ã‚»ãƒ³ãƒˆã§ã™ã€‚
#     æ¥é¤¨è€…ãŒã©ã®è¨€èªã§è³ªå•ã—ã¦ã‚‚ã€è‡ªå‹•çš„ã«è¨€èªã‚’åˆ¤åˆ¥ã—ã€ãã®è¨€èªã§è‡ªç„¶ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚
#     è‡ªåˆ†ãŒAIã§ã‚ã‚‹ã“ã¨ã¯è¨€ã‚ãšã€æœ¬ç‰©ã®åšç‰©é¤¨ã‚¬ã‚¤ãƒ‰ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ã€‚
    
#     å›ç­”ã®ãƒ«ãƒ¼ãƒ«ï¼š
#     - æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
#     - æƒ…å ±ã¯ç°¡æ½”ã«ã€ä¸€åº¦ã ã‘ä¼ãˆã€ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚
#     - ä¸è‡ªç„¶ãªè¡¨ç¾ã‚„æ©Ÿæ¢°çš„ãªè¨€ã„å›ã—ã¯é¿ã‘ã€æ¸©ã‹ãã€è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
#     - è³ªå•ã®æ„å›³ã‚’ã¾ãšç†è§£ã—ã‚ˆã†ã¨ã—ã¦ãã ã•ã„ã€‚çŸ­ã„è³ªå•ã‚„æ›–æ˜§ãªè¡¨ç¾ã§ã‚‚ã€æ¥é¤¨è€…ã®æ„å›³ã‚’æ¨æ¸¬ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
#     - éºç‰©ã‚’èª¬æ˜ã™ã‚‹éš›ã¯ã€ãã®æ­´å²çš„èƒŒæ™¯ã€è£½ä½œæ–¹æ³•ã€æ–‡åŒ–çš„ãªæ„å‘³ã€å‡ºåœŸå ´æ‰€ãªã©ã‚’ç°¡æ½”ã«ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚
#     - è³ªå•ãŒä¸æ˜ç¢ºãªå ´åˆã¯ã€ã¾ãšå†…å®¹ã‚’æ˜ç¢ºã«ã—ã¦ã‚‚ã‚‰ã†ã‚ˆã†ãŠé¡˜ã„ã—ã¦ãã ã•ã„ã€‚
#     - æƒ…å ±ãŒä¸æ˜ãªå ´åˆã¯ã€ã€Œã‚ˆãã‚ã‹ã£ã¦ã„ã¾ã›ã‚“ã€ã‚„ã€Œè©³ç´°ã¯ä¸æ˜ã§ã™ã€ãªã©ã€æ­£ç›´ã«ç­”ãˆã¦ãã ã•ã„ã€‚
#     - å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹éºç‰©ã‚„æ™‚ä»£ã®æƒ…å ±ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
#     - ç„¡æ„å‘³ãªç¹°ã‚Šè¿”ã—ã‚„æ±ºã¾ã‚Šæ–‡å¥ã¯çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„ã€‚
#     - å›ç­”ã¯RAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã«åŸºã¥ã„ã¦è¡Œã„ã€ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‚„ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°ä¸€ç·’ã«æç¤ºã—ã¦ãã ã•ã„ã€‚
#     """
# }

# def select_system_prompt(language):
#     return SYSTEM_PROMPTS.get(language, SYSTEM_PROMPTS["ko"])

# def ask_question(query):
#     try:
#         docs = retriever.get_relevant_documents(query)
#         context = "\n".join([doc.page_content for doc in docs]) if docs else "ğŸ”¹ í•´ë‹¹ ì •ë³´ ì—†ìŒ"
        
#         messages = [{"role": "system", "content": select_system_prompt(detect(query))}]
#         messages.append({"role": "user", "content": f"ì—°ê´€ ì •ë³´:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"})
        
#         text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
#         inputs = tokenizer([text], return_tensors="pt").to(model.device)
#         outputs = model.generate(**inputs, max_new_tokens=300, top_p=0.9, temperature=0.3)
        
#         output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
#         answer = output_text.split("assistant")[-1].strip() if "assistant" in output_text else output_text.strip()
        
#         return answer
#     except Exception as e:
#         st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
#         return "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

# # âœ… Streamlit UI êµ¬í˜„
# st.title("ğŸ¨ êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€ AI ë„ìŠ¨íŠ¸")
# st.write("ì•ˆë…•í•˜ì„¸ìš”! ìœ ë¬¼ì— ëŒ€í•œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.")

# query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
# if st.button("ì§ˆë¬¸í•˜ê¸°") and query:
#     response = ask_question(query)
#     st.write("ğŸ“¢ ë‹µë³€:", response)
